{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12077,"status":"ok","timestamp":1725478556331,"user":{"displayName":"Vadim Dunaevskiy","userId":"08243483562161920197"},"user_tz":-120},"id":"DPNZHJOLWQRZ","outputId":"bedc2c81-383e-4c57-bb70-bf4e319b57b3"},"outputs":[],"source":["!pip install colorednoise"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4099,"status":"ok","timestamp":1725478560423,"user":{"displayName":"Vadim Dunaevskiy","userId":"08243483562161920197"},"user_tz":-120},"id":"90mm1wfyqbwU"},"outputs":[],"source":["from google.colab import drive\n","import zipfile\n","import os\n","import pickle\n","import numpy as np\n","from tqdm import tqdm  #for the progress bar\n","import torch\n","import shutil\n","from pathlib import Path\n","import re\n","import matplotlib.pyplot as plt\n","import colorednoise as cn\n"]},{"cell_type":"markdown","metadata":{"id":"BP9XlsQXtKxv"},"source":["Connecting to Google Drive and Specifying Paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yk9_-5e5NZzI"},"outputs":[],"source":["# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Unzip the uploaded folder\n","#zip_path = '/content/drive/MyDrive/HRTF_noise/hr.zip'\n","unzip_path = '/content/drive/MyDrive/HRTF_noise/hrtf_folder/hr'\n","modified_folder_path = '/content/drive/MyDrive/HRTF_noise/brown_noise/-50dB/modified_hrtf_folder'\n","\n","if not os.path.exists(modified_folder_path):\n","    os.makedirs(modified_folder_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50ReVZt4Vfx2"},"outputs":[],"source":["def generate_noise(hrir_shape, noise_type):\n","    \"\"\"\n","    Generates multi-dimensional noise based on the input shape and noise type.\n","\n","    Args:\n","        hrir_shape (tuple): Shape of the desired noise (num_panels, panel_height, panel_width, num_freqs).\n","        noise_type (int): Type of noise to generate. 0 for white noise, 1 for pink noise, 2 for brown noise.\n","\n","    Returns:\n","        Generated noise tensor with the same shape as `hrir_shape`.\n","    \"\"\"\n","\n","    num_panels, panel_height, panel_width, num_freqs = hrir_shape\n","    noise = torch.empty(hrir_shape)\n","\n","    for i in range(num_panels):\n","        for j in range(panel_height):\n","            for k in range(panel_width):\n","                # Generate pink noise for each position separately (in the time domain)\n","                noise[i, j, k, :] = torch.tensor(cn.powerlaw_psd_gaussian(noise_type, num_freqs))\n","\n","    return noise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zeihujdXPJF"},"outputs":[],"source":["def add_noise_to_hrtf(hrir, noise, snr_db):\n","    \"\"\"Adds noise to the HRTF (in the frequency domain) with a specified signal-to-noise ratio (SNR),\n","       operating in the time domain.\n","\n","    Args:\n","        hrir: The HRIR tensor (assumed to be in the time domain).\n","        noise: The pink noise tensor (already in the time domain).\n","        snr_db: The desired signal-to-noise ratio in decibels.\n","\n","    Returns:\n","        The modified HRTF tensor with the added noise, back in the frequency domain.\n","    \"\"\"\n","    #for debugging purposes\n","    #print(f\"the shape of hrir is{hrir.shape} and that of noise {noise.shape}\")\n","\n","    # Calculate the RMS amplitude of the HRIR and noise in the time domain\n","    hrir_rms = torch.sqrt(torch.mean(hrir**2, dim=-1, keepdim=True))\n","    noise_rms = torch.sqrt(torch.mean(noise**2, dim=-1, keepdim=True))\n","\n","    # Convert desired SNR from dB to a linear scale (amplitude)\n","    snr_linear = 10 ** (snr_db / 20)\n","\n","    # Calculate the scaling factor for the noise\n","    #for debugging purposes\n","    #print(f\"the shape of hrir_rms is {hrir_rms.shape} and of noise is {noise_rms.shape}\")\n","    noise_scaling_factor = hrir_rms / (noise_rms * snr_linear)\n","\n","    # Apply the scaling factor to the noise\n","    noise_scaled = noise * noise_scaling_factor\n","\n","    #for debugging purposes\n","    #print(f\"the size of the hrir is {hrir.size} and the size of the noise is {noise.size}\")\n","\n","    # Add the scaled noise to the HRIR in the time domain\n","    modified_hrir = hrir + noise_scaled\n","\n","    # Convert the modified HRIR back to the frequency domain (HRTF)\n","    modified_hrtf = torch.abs(torch.fft.rfft(modified_hrir, dim=-1))\n","    #print(f\"the shape of the modified hrtf is {modified_hrtf.shape}\")\n","\n","    #for debugging purposes\n","    #if (modified_hrtf < 0).any():\n","    #  print(\"modified hrtf negative \")\n","\n","#    assert(modified_hrtf.shape == hrtf.shape), f\"Shape different: modified {modified_hrtf.shape} != original {hrtf.shape}\"\n","\n","    return modified_hrtf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXlC5hyYXVti"},"outputs":[],"source":["# Function to save the modified HRTF to a pickle file\n","def save_hrtf(hrtf, file_path):\n","    with open(file_path, 'wb') as file:\n","        if(pickle.dump(hrtf, file, protocol=4)):\n","          print(\"successful\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfjb76c82eVj"},"outputs":[],"source":["\"\"\"\n","Function taken from the original work non upsampling, which can be found at:\n","https://github.com/ahogg/HRTF-upsampling-with-a-generative-adversarial-network-using-a-gnomonic-equiangular-projection/tree/main/evaluation\n","\"\"\"\n","def merge_left_right_hrtfs(input_dir, output_dir):\n","    # Clear/Create directory\n","    shutil.rmtree(Path(output_dir), ignore_errors=True)\n","    Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","    hrtf_file_names = [os.path.join(input_dir, hrtf_file_name) for hrtf_file_name in os.listdir(input_dir)\n","                       if os.path.isfile(os.path.join(input_dir, hrtf_file_name))]\n","\n","    data_dict_left = {}\n","    data_dict_right = {}\n","    for f in hrtf_file_names:\n","\n","        file_ext = re.findall(re.escape(input_dir) + '/(.*)_[0-9]*[a-z]*.pickle$', f)[0]\n","\n","        with open(f, \"rb\") as file:\n","            data = pickle.load(file)\n","\n","        # add to dict for right ears\n","        if re.search(re.escape(input_dir)+'/.*_[0-9]*right.pickle$', f):\n","            subj_id = int(re.findall(re.escape(input_dir)+'/.*_([0-9]*)right.pickle$', f)[0])\n","            if file_ext not in data_dict_right:\n","                data_dict_right[file_ext] = {}\n","            data_dict_right[file_ext][subj_id] = data\n","        # add to dict for left ears\n","        elif re.search(re.escape(input_dir)+'/.*_[0-9]*left.pickle$', f):\n","            subj_id = int(re.findall(re.escape(input_dir)+'/.*_([0-9]*)left.pickle$', f)[0])\n","            if file_ext not in data_dict_left:\n","                data_dict_left[file_ext] = {}\n","            data_dict_left[file_ext][subj_id] = data\n","\n","    for file_ext in data_dict_right.keys():\n","        missing_subj_ids =  list(set(data_dict_right[file_ext].keys()) - set(data_dict_left[file_ext].keys()))\n","        if len(missing_subj_ids) > 0:\n","            print('Excluding subject IDs where both ears do not exist (IDs: %s)' % ', '.join(map(str, missing_subj_ids)))\n","            for missing_subj_id in missing_subj_ids:\n","                data_dict_right[file_ext].pop(missing_subj_id, None)\n","                data_dict_left[file_ext].pop(missing_subj_id, None)\n","\n","        for subj_id in data_dict_right[file_ext].keys():\n","            hrtf_r = data_dict_right[file_ext][subj_id]\n","            hrtf_l = data_dict_left[file_ext][subj_id]\n","            dimension = hrtf_r.ndim-1\n","            hrtf_merged = torch.cat((hrtf_l, hrtf_r), dim=dimension)\n","            with open('%s/%s_%s.pickle' % (output_dir, file_ext, subj_id), \"wb\") as file:\n","                pickle.dump(hrtf_merged, file,protocol=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":2299,"status":"error","timestamp":1723310721004,"user":{"displayName":"Vadim Dunaevskiy","userId":"08243483562161920197"},"user_tz":-120},"id":"bWkTjuq_XXnb","outputId":"26d415ce-e1a9-45b9-b74c-f9babe3d2ebb"},"outputs":[],"source":["\"\"\"\n","Main loop that goes through every HRTF in the train and valid folders\n","\"\"\"\n","\n","for subfolder in [\"train\", \"valid\"]:\n","    subfolder_path = os.path.join(unzip_path, subfolder)\n","    modified_subfolder_path = os.path.join(modified_folder_path, subfolder)\n","    merged_subfolder_path = os.path.join(modified_folder_path, f\"merged_{subfolder}\")\n","\n","    # Create directories if they don't exist\n","    os.makedirs(modified_subfolder_path, exist_ok=True)\n","    os.makedirs(merged_subfolder_path, exist_ok=True)\n","    for filename in tqdm(os.listdir(subfolder_path), desc=f\"Processing {subfolder}\"):\n","\n","        if filename.endswith(\".pickle\"):\n","            file_path = os.path.join(subfolder_path, filename)\n","            new_file_path = os.path.join(modified_subfolder_path, filename)\n","            try:\n","                with open(file_path, 'rb') as file:  # Use context manager to open file\n","                    hrtf = pickle.load(file)\n","\n","                # convert the HRTF to HRIR\n","                hrir = torch.fft.irfft(hrtf, dim=-1)\n","\n","                noise = generate_noise(hrir.shape, 2)\n","\n","                modified_hrtf = add_noise_to_hrtf(hrir, noise, snr_db=-50)\n","\n","                # Save the modified HRTF\n","                #save_hrtf(modified_hrtf, new_file_path)\n","            except (pickle.UnpicklingError, EOFError) as e:  # Catch errors during load\n","                print(f\"Error loading pickle file '{filename}': {e}\")\n","            except Exception as e:  # Catch other errors\n","                print(f\"Error processing file '{filename}': {e}\")\n","     #Merge left and right HRTFs after processing all files in the subfolder\n","    merge_left_right_hrtfs(modified_subfolder_path, merged_subfolder_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2066,"status":"ok","timestamp":1718376290056,"user":{"displayName":"Vadim Dunaevskiy","userId":"08243483562161920197"},"user_tz":-60},"id":"wJmD5qaATgJW","outputId":"179a1e4e-63ac-42e7-e05d-7284f5c4b7f1"},"outputs":[],"source":["\"\"\"\n","Code used to understand problems during the loading of pickle files. The error\n","encountered was that whenever a pickle file would load, there would be an invalid load key\n","error because the files were opened on macOS, which created an invisible .DS_STORE file.\n","\"\"\"\n","import os\n","import pickle\n","\n","def load_hrtf(file_path):\n","    with open(file_path, 'rb') as file:\n","        hrtf = pickle.load(file)\n","    return hrtf\n","\n","def verify_pickle_files(directory):\n","    count = 0\n","    for filename in os.listdir(directory):\n","        if filename.endswith('.pickle'):\n","            file_path = os.path.join(directory, filename)\n","            try:\n","                hrtf = load_hrtf(file_path)\n","                count+=1\n","                print(count)\n","                print(f\"Successfully loaded {file_path}\")\n","            except pickle.UnpicklingError as e:\n","                print(f\"Failed to load {file_path}: {e}\")\n","            except Exception as e:\n","                print(f\"An unexpected error occurred while loading {file_path}: {e}\")\n","\n","# Change this to the directory containing your pickle files\n","pickle_directory = '/content/drive/MyDrive/HRTF_noise/pink_noise/0dB/modified_hrtf_folder/merged_train'\n","verify_pickle_files(pickle_directory)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPGIXaS6GpLShnX6WciiFYk","mount_file_id":"1kYzlrQvqJ_FJ7wGZ2jiVPWspOTX8HMqQ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
